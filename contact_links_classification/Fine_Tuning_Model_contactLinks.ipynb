{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b283deb4-60b9-42df-8f91-b3b031fd3424",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch==2.2.2 \n",
    "!pip install transformers==4.32.1 \n",
    "!pip install seqeval==1.2.2 \n",
    "!pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85780b87-6516-4dda-b3ff-fe7587fafc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "import logging\n",
    "import time\n",
    "import os\n",
    "import csv\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c02e759e-3233-44c9-98d1-602875250fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContactLinkModel:\n",
    "    \n",
    "    def __init__(self, model_name='bert-base-cased', num_labels=2, max_length=40):\n",
    "        self.model_name = model_name\n",
    "        self.num_labels = num_labels\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.linkProcessing = LinkProcessing()\n",
    "        \n",
    "    def load_from_huggingface(self):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(self.model_name)\n",
    "        self.model = BertForSequenceClassification.from_pretrained(\n",
    "            self.model_name,\n",
    "            num_labels= self.num_labels\n",
    "        )\n",
    "\n",
    "    def load_from_local(self, tokenizer_path='bert-base-cased', model_path='./Models/model_0/model_contact_40_maxlen_10_epochs'):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(tokenizer_path)\n",
    "        self.model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "    \n",
    "    def preprocess(self, texts, truncation=True, padding=True):\n",
    "        return self.tokenizer(\n",
    "            texts, \n",
    "            padding=padding,\n",
    "            truncation=truncation,\n",
    "            max_length=self.max_length, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "    def get_original_tokens(self, input_ids):\n",
    "        tokens = self.tokenizer.convert_ids_to_tokens(input_ids)\n",
    "        return tokens\n",
    "\n",
    "    def compute_metrics(self, preds, labels):\n",
    "        preds = preds.argmax(-1)\n",
    "        accuracy = accuracy_score(labels, preds)\n",
    "        recall = recall_score(labels, preds, average='binary')\n",
    "        precision = precision_score(labels, preds, average='binary')\n",
    "        f1 = f1_score(labels, preds, average='binary')\n",
    "        return accuracy, precision, recall, f1\n",
    "\n",
    "    def train(self, train_texts, train_labels, val_texts, val_labels, num_epochs, batch_size):\n",
    "        \n",
    "        os.makedirs(\"./Models/model_0\", exist_ok=True)\n",
    "        model_save_path = f\"./Models/model_0/model_contact_{self.max_length}_maxlen_{num_epochs}_epochs\"\n",
    "\n",
    "        train_encodings = self.preprocess(train_texts)\n",
    "        val_encodings = self.preprocess(val_texts)\n",
    "\n",
    "        train_dataset = Dataset(train_encodings, train_labels)\n",
    "        val_dataset = Dataset(val_encodings, val_labels)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(device)\n",
    "        \n",
    "        optimizer = AdamW(self.model.parameters(), lr=2e-5)\n",
    "        total_steps = len(train_loader) * num_epochs\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "\n",
    "        csv_filename = f\"{model_save_path}_info.csv\"\n",
    "        header = [\"training_details\"]\n",
    "        is_empty = True\n",
    "        \n",
    "        start_time = time.time()\n",
    "        for epoch in range(num_epochs):\n",
    "            self.model.train()\n",
    "            total_loss = 0\n",
    "            for batch in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                outputs = self.model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                total_loss += loss.item()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "\n",
    "            avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "            # Validation\n",
    "            self.model.eval()\n",
    "            val_preds, val_labels = [], []\n",
    "            val_total_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for batch in val_loader:\n",
    "                    input_ids = batch['input_ids'].to(device)\n",
    "                    attention_mask = batch['attention_mask'].to(device)\n",
    "                    labels = batch['labels'].to(device)\n",
    "                    outputs = self.model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                    logits = outputs.logits\n",
    "                    val_preds.extend(logits.detach().cpu().numpy())\n",
    "                    val_labels.extend(labels.cpu().numpy())\n",
    "                    loss = outputs.loss\n",
    "                    val_total_loss += loss.item()\n",
    "\n",
    "            avg_val_loss = val_total_loss / len(val_loader)\n",
    "\n",
    "            val_preds = np.array(val_preds)\n",
    "            val_labels = np.array(val_labels)\n",
    "            accuracy, precision, recall, f1 = self.compute_metrics(val_preds, val_labels)\n",
    "\n",
    "            training_details = (f\"Époque {epoch+1}/{num_epochs} - Train Loss: {avg_train_loss:.4f} - \"\n",
    "                                f\"Validation Loss: {avg_val_loss:.4f} - Validation Accuracy: {accuracy:.4f} - \"\n",
    "                                f\"Precision: {precision:.4f} - Recall: {recall:.4f} - F1 Score: {f1:.4f}\")\n",
    "            print(training_details)\n",
    "        \n",
    "            with open(csv_filename, 'a', newline='', encoding='utf-8-sig') as csvfile:\n",
    "                csv_writer = csv.writer(csvfile)\n",
    "                if is_empty:\n",
    "                    csv_writer.writerow(header)\n",
    "                    is_empty = False\n",
    "                csv_writer.writerow([training_details])\n",
    "\n",
    "        end_time = time.time()\n",
    "        total_fine_tuning_time = end_time - start_time\n",
    "        \n",
    "        training_details = f\"Fine-tuning terminé! Temps total: {total_fine_tuning_time:.2f} secondes | {total_fine_tuning_time/60:.2f} min | {total_fine_tuning_time/3600:.2f} hours\"\n",
    "        print(training_details)\n",
    "\n",
    "        # Enregistrer le modèle finetuné\n",
    "        self.model.save_pretrained(model_save_path)\n",
    "        print(f\"Modèle enregistré à {model_save_path}\")\n",
    "        \n",
    "        with open(csv_filename, 'a', newline='', encoding='utf-8-sig') as csvfile:\n",
    "            csv_writer = csv.writer(csvfile)\n",
    "            csv_writer.writerow([training_details])\n",
    "            csv_writer.writerow([f\"Modèle enregistré à {model_save_path}\"])\n",
    "\n",
    "\n",
    "    def predict(self, text):\n",
    "        inputs = self.preprocess([text])\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        \n",
    "        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        predictions = predictions.cpu().detach().numpy()\n",
    "        predictions = np.argmax(predictions)\n",
    "        return predictions\n",
    "\n",
    "    def predict_label_links(self, cleaned_links):\n",
    "        link_name_label = []\n",
    "        for i in range(len(cleaned_links)):\n",
    "            predictions = self.predict(cleaned_links[i][1])\n",
    "            link_name_label.append((cleaned_links[i][0], cleaned_links[i][1], predictions))\n",
    "        return link_name_label\n",
    "        \n",
    "    def get_contact_links(self, htmlContent):\n",
    "        links = self.linkProcessing.preprocess_links(htmlContent)\n",
    "        predictedLinks = self.predict_label_links(links)\n",
    "        return [link for link in predictedLinks if link[2]==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e2b3c84-675f-400f-af28-761592f8b895",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels=None):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        if self.labels is not None:\n",
    "            item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ec2e248-075b-465b-b90e-8046b73a737d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinkProcessing :\n",
    "    def __init__(self):\n",
    "        self.max_len_link_name = 6\n",
    "        \n",
    "    def extract_links(self, contenu_html):\n",
    "        soup = BeautifulSoup(contenu_html, 'html.parser')\n",
    "        links = []\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            href = link['href']\n",
    "            text = link.get_text(strip=True)\n",
    "            links.append((href, text))\n",
    "        return links\n",
    "\n",
    "    def remove_empty_links(self, links):\n",
    "        cleaned_links = [(href, text) for href, text in links if href.strip() not in (\"#\", \"\")]\n",
    "        return cleaned_links\n",
    "\n",
    "    def filter_valid_name_links(self, links):\n",
    "        cleaned_links = [(href, text) for href, text in links if text.strip() and len(text.split()) <= self.max_len_link_name]\n",
    "        return cleaned_links\n",
    "\n",
    "    def preprocess_links(self, contenu_html):\n",
    "        links = self.extract_links(contenu_html)\n",
    "        cleaned_links = self.remove_empty_links(links)\n",
    "        cleaned_links = self.filter_valid_name_links(cleaned_links)\n",
    "        return cleaned_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33d65c0e-457a-44f0-9680-2fdf2c9d2ff1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link_name</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>contact</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>contactez-nous</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Contactez nous</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nous contacter</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>الإتصال</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9948</th>\n",
       "      <td>الاتصال بنا</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9949</th>\n",
       "      <td>اتصل بنا</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9950</th>\n",
       "      <td>من نحن</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9951</th>\n",
       "      <td>إتصل بنا</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9952</th>\n",
       "      <td>Nous contacter</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9953 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           link_name  label\n",
       "0            contact      1\n",
       "1     contactez-nous      1\n",
       "2     Contactez nous      1\n",
       "3     nous contacter      1\n",
       "4            الإتصال      1\n",
       "...              ...    ...\n",
       "9948     الاتصال بنا      1\n",
       "9949        اتصل بنا      1\n",
       "9950          من نحن      1\n",
       "9951        إتصل بنا      1\n",
       "9952  Nous contacter      1\n",
       "\n",
       "[9953 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('./data/LINK_CONTACT_DATA_2.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0db5adec-ce14-4a7c-b9f6-621cfc72c6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = list(data[\"link_name\"])\n",
    "y = list(data[\"label\"])\n",
    "\n",
    "# Split data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "302b645a-413f-4ac0-b164-1bbd4b457290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "model_name = 'bert-base-cased'\n",
    "num_labels = 2\n",
    "max_length = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7da4b2ae-0494-49e6-a948-af2b17d1f25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6970b671-5c70-45b7-89fe-dfd02e10700f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the classifier\n",
    "classifier = ContactLinkModel(model_name, num_labels, max_length)\n",
    "classifier.load_from_huggingface()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cabf83-1518-473d-a17b-a1263ef5dfda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "C:\\Users\\Windows\\AppData\\Local\\Temp\\ipykernel_7940\\3573251049.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "classifier.train(X_train, y_train, X_val, y_val, num_epochs=30, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef92156-a098-45f0-9b57-3a8447bbfdb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c26ae43-b887-467b-b0ec-5823edb768b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db4783e5-04dc-471e-863b-43595b39d996",
   "metadata": {},
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d78f7ca-cc3d-4d67-8938-499cb82bd3de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.45.1-py3-none-any.whl.metadata (44 kB)\n",
      "     ---------------------------------------- 0.0/44.4 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/44.4 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/44.4 kB ? eta -:--:--\n",
      "     ----------------------------------- -- 41.0/44.4 kB 326.8 kB/s eta 0:00:01\n",
      "     -------------------------------------- 44.4/44.4 kB 313.2 kB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in c:\\anaconda\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers)\n",
      "  Downloading huggingface_hub-0.25.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\anaconda\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\anaconda\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\anaconda\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\anaconda\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\anaconda\\lib\\site-packages (from transformers) (2.32.2)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.5-cp312-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers)\n",
      "  Downloading tokenizers-0.20.0-cp312-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\anaconda\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\anaconda\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\anaconda\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\anaconda\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\anaconda\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\anaconda\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\anaconda\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\anaconda\\lib\\site-packages (from requests->transformers) (2024.7.4)\n",
      "Downloading transformers-4.45.1-py3-none-any.whl (9.9 MB)\n",
      "   ---------------------------------------- 0.0/9.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/9.9 MB 6.4 MB/s eta 0:00:02\n",
      "    --------------------------------------- 0.2/9.9 MB 2.1 MB/s eta 0:00:05\n",
      "    --------------------------------------- 0.2/9.9 MB 1.7 MB/s eta 0:00:06\n",
      "   - -------------------------------------- 0.3/9.9 MB 1.5 MB/s eta 0:00:07\n",
      "   - -------------------------------------- 0.4/9.9 MB 1.8 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 0.5/9.9 MB 1.8 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 0.6/9.9 MB 1.9 MB/s eta 0:00:05\n",
      "   -- ------------------------------------- 0.7/9.9 MB 1.8 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 0.7/9.9 MB 1.9 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 0.8/9.9 MB 1.9 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 0.9/9.9 MB 1.8 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 1.0/9.9 MB 1.8 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 1.0/9.9 MB 1.7 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 1.0/9.9 MB 1.7 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 1.1/9.9 MB 1.7 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 1.1/9.9 MB 1.7 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 1.1/9.9 MB 1.5 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 1.2/9.9 MB 1.5 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 1.2/9.9 MB 1.5 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 1.2/9.9 MB 1.4 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 1.3/9.9 MB 1.4 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 1.3/9.9 MB 1.4 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 1.4/9.9 MB 1.4 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 1.5/9.9 MB 1.4 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 1.6/9.9 MB 1.4 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 1.7/9.9 MB 1.5 MB/s eta 0:00:06\n",
      "   ------- -------------------------------- 1.8/9.9 MB 1.5 MB/s eta 0:00:06\n",
      "   ------- -------------------------------- 1.9/9.9 MB 1.6 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 2.1/9.9 MB 1.6 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 2.1/9.9 MB 1.6 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 2.3/9.9 MB 1.7 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 2.4/9.9 MB 1.7 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 2.6/9.9 MB 1.8 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 2.6/9.9 MB 1.8 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 2.7/9.9 MB 1.8 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 2.9/9.9 MB 1.8 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 3.0/9.9 MB 1.9 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 3.0/9.9 MB 1.8 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 3.1/9.9 MB 1.8 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 3.1/9.9 MB 1.8 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 3.3/9.9 MB 1.8 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 3.5/9.9 MB 1.9 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 3.5/9.9 MB 1.9 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 3.6/9.9 MB 1.8 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 3.7/9.9 MB 1.9 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 3.9/9.9 MB 2.0 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 4.1/9.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 4.1/9.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 4.2/9.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 4.2/9.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 4.2/9.9 MB 1.9 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 4.2/9.9 MB 1.9 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 4.2/9.9 MB 1.9 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 4.3/9.9 MB 1.8 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 4.4/9.9 MB 1.8 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 4.5/9.9 MB 1.8 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 4.6/9.9 MB 1.8 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 4.7/9.9 MB 1.8 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 4.9/9.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 4.9/9.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 4.9/9.9 MB 1.9 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 5.0/9.9 MB 1.8 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 5.2/9.9 MB 1.8 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 5.3/9.9 MB 1.9 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 5.3/9.9 MB 1.8 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 5.7/9.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 5.7/9.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 5.9/9.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 6.1/9.9 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 6.2/9.9 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 6.2/9.9 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 6.2/9.9 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 6.2/9.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 6.3/9.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 6.4/9.9 MB 1.9 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 6.5/9.9 MB 1.9 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 6.6/9.9 MB 1.9 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 6.8/9.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 7.0/9.9 MB 2.0 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 7.2/9.9 MB 2.0 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 7.3/9.9 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 7.5/9.9 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 7.6/9.9 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 7.8/9.9 MB 2.1 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 8.0/9.9 MB 2.1 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 8.2/9.9 MB 2.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 8.3/9.9 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 8.4/9.9 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 8.6/9.9 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 8.6/9.9 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 8.6/9.9 MB 2.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 8.7/9.9 MB 2.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 8.7/9.9 MB 2.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 8.7/9.9 MB 2.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 8.7/9.9 MB 2.0 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 8.7/9.9 MB 2.0 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 8.7/9.9 MB 2.0 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 8.7/9.9 MB 2.0 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 8.7/9.9 MB 2.0 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 8.7/9.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 8.8/9.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 8.8/9.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 8.8/9.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 8.8/9.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 8.8/9.9 MB 1.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 8.8/9.9 MB 1.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 8.8/9.9 MB 1.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 8.8/9.9 MB 1.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 8.8/9.9 MB 1.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 8.8/9.9 MB 1.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 8.8/9.9 MB 1.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 8.9/9.9 MB 1.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 8.9/9.9 MB 1.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 9.0/9.9 MB 1.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 9.1/9.9 MB 1.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 9.2/9.9 MB 1.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 9.3/9.9 MB 1.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 9.3/9.9 MB 1.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 9.4/9.9 MB 1.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 9.4/9.9 MB 1.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 9.5/9.9 MB 1.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 9.5/9.9 MB 1.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 9.6/9.9 MB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.7/9.9 MB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.7/9.9 MB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.8/9.9 MB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.9/9.9 MB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.9/9.9 MB 1.7 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.25.1-py3-none-any.whl (436 kB)\n",
      "   ---------------------------------------- 0.0/436.4 kB ? eta -:--:--\n",
      "   ----------- ---------------------------- 122.9/436.4 kB 3.5 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 204.8/436.4 kB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 276.5/436.4 kB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 409.6/436.4 kB 2.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  430.1/436.4 kB 2.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 436.4/436.4 kB 1.8 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.4.5-cp312-none-win_amd64.whl (286 kB)\n",
      "   ---------------------------------------- 0.0/286.3 kB ? eta -:--:--\n",
      "   --------------------------------------- 286.3/286.3 kB 17.3 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.20.0-cp312-none-win_amd64.whl (2.3 MB)\n",
      "   ---------------------------------------- 0.0/2.3 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.1/2.3 MB 2.4 MB/s eta 0:00:01\n",
      "   ---- ----------------------------------- 0.3/2.3 MB 3.2 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 0.4/2.3 MB 2.7 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 0.6/2.3 MB 3.2 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 0.7/2.3 MB 3.2 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 0.9/2.3 MB 3.3 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 1.0/2.3 MB 3.2 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 1.1/2.3 MB 3.2 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 1.2/2.3 MB 3.0 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 1.4/2.3 MB 3.0 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.5/2.3 MB 3.0 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.6/2.3 MB 3.0 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.8/2.3 MB 3.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 2.0/2.3 MB 3.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 2.1/2.3 MB 3.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 2.2/2.3 MB 3.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 2.2/2.3 MB 3.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.3/2.3 MB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.3/2.3 MB 2.7 MB/s eta 0:00:00\n",
      "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.25.1 safetensors-0.4.5 tokenizers-0.20.0 transformers-4.45.1\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6bcf96a-1621-432a-93bb-d66d5e00ba34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('https://www.hespress.com/contact1', 'اعمل معنا'),\n",
       " ('https://www.hespress.com/contact1', 'contactez-nous'),\n",
       " ('https://www.hespress.com/contact2', 'contact'),\n",
       " ('https://www.hespress.com/contact2', 'Informations de contact'),\n",
       " ('https://www.hespress.com/contact2', 'هيئة التحرير'),\n",
       " ('https://www.hespress.com/contact2', 'من نحن؟'),\n",
       " ('https://www.hespress.com/contact2', 'اتصال')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ContactLinkModel import ContactLinkModel\n",
    "\n",
    "classifier = ContactLinkModel()\n",
    "classifier.load_from_local(model_path='./Models/model_0/model_contact_40_maxlen_10_epochs')\n",
    "\n",
    "text = \"\"\"<a class=\"nav-link\" href=\"https://www.hespress.com/contact1\"><div>اعمل معنا<div></a>\n",
    "<a class=\"nav-link\" href=\"https://www.hespress.com/contact1\"><div>contactez-nous<div></a>\n",
    "<a class=\"nav-link\" href=\"#\"><div>aaa<div></a>\n",
    "<a class=\"nav-link\" href=\"https://www.hespress.com/contact1\"><div>bbb<div></a>\n",
    "        <li class=\"menu-item nav-item\"><a class=\"nav-link\" href=\"https://jdjd/jd\">cc cc cc cc cc cc cc cc</a>\n",
    "        <a class=\"nav-link\" href=\"https://www.hespress.com/contact2\">text text</a>\n",
    "        <a class=\"nav-link\" href=\"https://www.hespress.com/contact2\">contact</a>\n",
    "        <a class=\"nav-link\" href=\"https://www.hespress.com/contact2\">Informations de contact</a>\n",
    "        <a class=\"nav-link\" href=\"https://www.hespress.com/contact2\">هيئة التحرير</a>\n",
    "        <a class=\"nav-link\" href=\"https://www.hespress.com/contact2\">من نحن؟</a>\n",
    "        <a class=\"nav-link\" href=\"https://www.hespress.com/contact2\">اتصال</a>\n",
    "        <a class=\"nav-link\" href=\"https://www.hespress.com/contact2\">economie</a>\n",
    "        </li>\"\"\"\n",
    "\n",
    "test = classifier.get_contact_links(text)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0feb04bc-a351-4225-8a6f-6ec3ac035d0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b74ae26-02cd-4439-ab2a-4f2248553920",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
